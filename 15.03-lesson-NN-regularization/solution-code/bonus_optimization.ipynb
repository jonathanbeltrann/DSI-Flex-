{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## Bonus: Optimization\n",
    "\n",
    "_Authors:_ Matt Brems, Tim Book, Justin Pounders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "---\n",
    "We're going to talk about ways to speed up the process of optimization. Surprisingly, local optima are not often problems with neural networks; there's a much larger issue with \"plateaus,\" or areas where the derivative is approximately 0. This makes learning much slower.\n",
    "\n",
    "## Quick Fixes:\n",
    "1. **Feature Scaling:** As we've discussed before, feature scaling can speed up the process of gradient descent. Because gradient descent works \"geometrically,\" the scales of our $w$ values have a large impact on how quickly our parameters converge to the true value. Since we're working with many parameters (often 1,000 or more), scaling our features will speed up optimization.\n",
    "\n",
    "2. **Mini-batch Gradient Descent:** In `sklearn`, we use `.fit()` to estimate the parameters in our model. We do the same in neural networks, but if we're working with a very large data set (as is common in neural networks), passing data through our network will cause learning to be slow. By specifying a `batch_size` within the `.fit()` method, we can expedite our learning.\n",
    "    - `batch_size`: Integer or `None`. Number of samples per gradient update. If unspecified, it will default to 32.\n",
    "    \n",
    "**Note:** Mini batches will usually be a power of 2 (32, 64, 128, 256, 512) due to the fact that computers operate in base 2.\n",
    "\n",
    "- [Overview of the three types of gradient descent: batch, stochastic, mini-batch](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='momentum'></a>\n",
    "## Gradient Descent with Momentum\n",
    "\n",
    "One problem we notice with mini-batch gradient descent compared to batch gradient descent is that it tends to oscillate more, due to the fact that at each iteration it is being fed less training data than batch gradient descent.\n",
    "\n",
    "![](../assets/grad-descent.png)\n",
    "![](../assets/mini-grad-descent.png)\n",
    "\n",
    "[*source*](http://pengcheng.tech/2017/09/28/gradient-descent-momentum-and-adam/)\n",
    "\n",
    "Ideally, we'd like to take advantage of the speed and memory efficiency of mini-batch gradient descent, without the oscillations. We can use the idea of **momentum** to help us out here. (Momentum is based on [exponentially weighted moving averages](https://www.compose.com/articles/metrics-maven-calculating-an-exponentially-weighted-moving-average-in-postgresql/), which causes our oscillations to largely cancel each other out.) \n",
    "\n",
    "Typically, when updating our parameters, we'll follow this formula:\n",
    "\n",
    "$$W = W -\\alpha\\frac{\\partial \\text{loss}}{\\partial W}$$  \n",
    "$$b = b - \\alpha\\frac{\\partial \\text{loss}}{\\partial b}$$\n",
    "\n",
    "We're going to modify this formula to this form:\n",
    "\n",
    "$$W = W -\\alpha V_{\\partial W}$$  \n",
    "$$b = b - \\alpha V_{\\partial b}$$\n",
    "\n",
    "Where \n",
    "$$V_{\\partial W} = \\beta V_{\\partial W - 1} + (1-\\beta)\\frac{\\partial \\text{loss}}{\\partial W}$$\n",
    "and\n",
    "$$V_{\\partial b} = \\beta V_{\\partial b - 1} + (1-\\beta)\\frac{\\partial \\text{loss}}{\\partial b}$$\n",
    "\n",
    "When implementing gradient descent with momentum, you'll have two hyperparameters, $\\alpha$ and $\\beta$. From a practical point of view, $\\beta$ is typically $0.9$, but you can test out other values if you'd like.\n",
    "\n",
    "### ADAM Optimization\n",
    "\n",
    "The optimization algorithm you'll likely use when implementing your feed-forward neural network is called ADAM (Adaptive Moment Estimation). It is a combination of gradient descent with momentum and another optimization method called RMSProp (Root Mean Square Propagation). For the sake of this lesson, we won't cover how ADAM works, but with gradient descent with momentum as building block, the ADAM optimization is not too far off, as it largely relies on the concept of momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "One method of minimizing the risk of overfitting is to gather more data. While this is usually very costly, we may sometimes be able to take our existing data to generate substantially more data.\n",
    "- Images: Reflect, crop, random rotations or distortions, adjust lighting.\n",
    "    - [The Effectiveness of Data Augmentation in Image Classification using Deep Learning](http://cs231n.stanford.edu/reports/2017/pdfs/300.pdf)\n",
    "- Non-Images: [SMOTE](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/) (Synthetic Minority Over-Sampling Technique)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
